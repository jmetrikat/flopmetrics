{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from metriwatt.profiler import TorchProfiler\n",
    "from metriwatt.ncu import NCUProfiler\n",
    "from metriwatt.network import ToyNetwork, run_toy_network_forward_ncu, run_toy_network_forward_backward_ncu, construct_toy_network_and_input_for_ncu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_network_forward_flops(dim, n_layers, n_tokens):\n",
    "    with TorchProfiler() as prof:\n",
    "        net = ToyNetwork(n_layers=n_layers, dim=dim)\n",
    "        x = torch.randn(dim, n_tokens, device=\"cuda\")\n",
    "        with prof.record_context(\"forward\"):\n",
    "            _ = net(x)\n",
    "    return prof.get_flops_by_step().loc[\"forward\", \"flops\"]\n",
    "\n",
    "\n",
    "def toy_network_backward_flops(dim, n_layers, n_tokens):\n",
    "    with TorchProfiler() as prof:\n",
    "        net = ToyNetwork(n_layers=n_layers, dim=dim)\n",
    "        x = torch.randn(dim, n_tokens, device=\"cuda\")\n",
    "        y = net(x)\n",
    "        with prof.record_context(\"backward\"):\n",
    "            y.sum().backward()\n",
    "    return prof.get_flops_by_step().loc[\"backward\", \"flops\"]\n",
    "\n",
    "\n",
    "def toy_network_forward_flops_ncu(dim, n_layers, n_tokens):\n",
    "    ncu = NCUProfiler()\n",
    "    _ = ncu.profile_function(run_toy_network_forward_ncu, {\n",
    "        \"dim\": dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_tokens\": n_tokens,\n",
    "    })\n",
    "    flops = ncu.get_total_flops()\n",
    "\n",
    "    ncu2 = NCUProfiler()\n",
    "    ncu2.profile_function(construct_toy_network_and_input_for_ncu, {\n",
    "        \"dim\": dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_tokens\": n_tokens,\n",
    "    })\n",
    "    setup_flops = ncu2.get_total_flops()\n",
    "    flops -= setup_flops\n",
    "\n",
    "    ncu.result.to_csv(\"experiments/toy_network/results/toy_network_forward_flops_ncu.csv\")\n",
    "    ncu2.result.to_csv(\"experiments/toy_network/results/toy_network_setup_flops_ncu.csv\")\n",
    "\n",
    "    return flops\n",
    "\n",
    "\n",
    "def toy_network_forward_backward_flops_ncu(dim, n_layers, n_tokens):\n",
    "    ncu = NCUProfiler()\n",
    "    _ = ncu.profile_function(run_toy_network_forward_backward_ncu, {\n",
    "        \"dim\": dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_tokens\": n_tokens,\n",
    "    })\n",
    "    flops = ncu.get_total_flops()\n",
    "\n",
    "    ncu.result.to_csv(\"experiments/toy_network/results/toy_network_forward_backward_flops_ncu.csv\")\n",
    "\n",
    "    ncu2 = NCUProfiler()\n",
    "    ncu2.profile_function(construct_toy_network_and_input_for_ncu, {\n",
    "        \"dim\": dim,\n",
    "        \"n_layers\": n_layers,\n",
    "        \"n_tokens\": n_tokens,\n",
    "    })\n",
    "    setup_flops = ncu2.get_total_flops()\n",
    "    flops -= setup_flops\n",
    "\n",
    "    return flops\n",
    "\n",
    "\n",
    "def toy_network_params(dim, n_layers):\n",
    "    net = ToyNetwork(n_layers=n_layers, dim=dim)\n",
    "    return sum(p.numel() for p in net.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10      # number of layers\n",
    "D = 1024    # size of input/output dimension\n",
    "M = 128     # sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10485760\n",
      "                                                 forward_flops  backward_flops\n",
      "torch profiler (experimental)                    2,684,354,560   5,100,273,664\n",
      "ncu profiler (experimental)                      2,727,606,309   5,134,953,113\n",
      "theoretical (analyzed)                           2,684,354,560   5,100,273,664\n",
      "theoretical exact (analyzed)                     2,685,665,280   5,102,764,032\n",
      "theoretical exact gpu implementation (analyzed)  2,685,665,280   5,102,764,032\n"
     ]
    }
   ],
   "source": [
    "baseline_num_params = toy_network_params(D, N)\n",
    "print(baseline_num_params)\n",
    "\n",
    "baseline_forward_flops_exp = toy_network_forward_flops(D, N, M)\n",
    "baseline_backward_flops_exp = toy_network_backward_flops(D, N, M)\n",
    "\n",
    "baseline_forward_flops_ncu = toy_network_forward_flops_ncu(D, N, M)\n",
    "baseline_backward_flops_ncu = toy_network_forward_backward_flops_ncu(D, N, M) - baseline_forward_flops_ncu\n",
    "\n",
    "baseline_forward_flops_theory = N * (2 * D * D * M)\n",
    "baseline_backward_flops_theory = (N - 1) * (4 * D * D * M) + 2 * D * D * M\n",
    "\n",
    "baseline_forward_flops_theory_exact = baseline_forward_flops_theory + N * (D * M)\n",
    "baseline_backward_flops_theory_exact = baseline_backward_flops_theory + (N - 1) * (2 * D * M) + D * M\n",
    "\n",
    "baseline_forward_flops_theory_exact_gpu = baseline_forward_flops_theory_exact # TODO: add actual equation\n",
    "baseline_backward_flops_theory_exact_gpu = baseline_backward_flops_theory_exact # TODO: add actual equation\n",
    "\n",
    "baseline_df = pd.DataFrame(\n",
    "    {\n",
    "        \"forward_flops\": [baseline_forward_flops_exp, baseline_forward_flops_ncu, baseline_forward_flops_theory, baseline_forward_flops_theory_exact, baseline_forward_flops_theory_exact_gpu],\n",
    "        \"backward_flops\": [baseline_backward_flops_exp, baseline_backward_flops_ncu, baseline_backward_flops_theory, baseline_backward_flops_theory_exact, baseline_backward_flops_theory_exact_gpu],\n",
    "    },\n",
    "    index=[\"torch profiler (experimental)\", \"ncu profiler (experimental)\", \"theoretical (analyzed)\", \"theoretical exact (analyzed)\", \"theoretical exact gpu implementation (analyzed)\"],\n",
    ")\n",
    "pd.set_option('display.float_format', '{:,.0f}'.format)\n",
    "print(baseline_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
